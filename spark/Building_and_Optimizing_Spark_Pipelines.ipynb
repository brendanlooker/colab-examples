{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendanlooker/colab-examples/blob/main/spark/Building_and_Optimizing_Spark_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building and Optimizing Spark Pipelines\n",
        "This hands-on lab explores key optimization and maintenance strategies for using Apache Spark and Apache Iceberg. You will learn how to:\n",
        "\n",
        "- Tune Spark to leverage the performance benefits of Broadcast Hash Join over Shuffle Sort-Merge Join when dealing with large fact tables and small dimension tables.\n",
        "\n",
        "- Perform Iceberg file compaction to merge small files created by continuous writes.\n",
        "\n",
        "- Build a Structured Streaming pipeline that uses Iceberg's transactional MERGE INTO capability to perform real-time updates and inserts into a target table"
      ],
      "metadata": {
        "id": "q_kuJtb9bnnw"
      },
      "id": "q_kuJtb9bnnw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "1. Create a Spark Serverless Runtime"
      ],
      "metadata": {
        "id": "NvNy06ThX03F"
      },
      "id": "NvNy06ThX03F"
    },
    {
      "cell_type": "code",
      "id": "zzY5qeVFMEy48djG0l2o43bl",
      "metadata": {
        "tags": [],
        "id": "zzY5qeVFMEy48djG0l2o43bl",
        "colab": {
          "height": 316
        },
        "outputId": "a1fc51d5-d84d-4c0b-e5c8-51e2e02e8a0e",
        "collapsed": true
      },
      "source": [
        "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# --- Configuration Constants (Using user's provided values) ---\n",
        "CATALOG_NAME = \"my_catalog\"\n",
        "NAMESPACE = \"spark_lab\"\n",
        "PROJECT_ID = \"oh-lab-477016\"\n",
        "REGION = \"us-central1\"\n",
        "WAREHOUSE_DIR = \"gs://oh-lab-477016-warehouse/warehouse\"\n",
        "CHECKPOINT_DIR = \"gs://oh-lab-477016-checkpoints/checkpoint\"\n",
        "\n",
        "# --- Spark Session Initialization (Based on user's code) ---\n",
        "spark = DataprocSparkSession.builder \\\n",
        "    .appName(\"spark-lab\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(f\"spark.sql.catalog.{CATALOG_NAME}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(f\"spark.sql.catalog.{CATALOG_NAME}.catalog-impl\", \"org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\") \\\n",
        "    .config(f\"spark.sql.catalog.{CATALOG_NAME}.gcp_project\", f\"{PROJECT_ID}\") \\\n",
        "    .config(f\"spark.sql.catalog.{CATALOG_NAME}.gcp_location\", f\"{REGION}\") \\\n",
        "    .config(f\"spark.sql.catalog.{CATALOG_NAME}.warehouse\", f\"{WAREHOUSE_DIR}\") \\\n",
        "    .config(\"spark.executor.instances\", \"8\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"100\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"false\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.cloud.dataproc_spark_connect'",
          "traceback": [
            "\u001b[0;31mModuleNotFoundError:\u001b[0m No module named 'google.cloud.dataproc_spark_connect'",
            "",
            "\nConsider using a custom runtime: go/colab_binary",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-521864909.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataproc_spark_connect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataprocSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# --- Configuration Constants (Using user's provided values) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.cloud.dataproc_spark_connect'"
          ],
          "debug": {
            "argv": [
              "/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pool_default_78_gvisor.kernel.colaboratory-playground.1496279041415.14b334fb3717c109/mount/server/notebook.par",
              "kernel",
              "-f",
              "/tmp/ipy-be-jc1xlne9/profile_colab/security/kernel-76608f24-b707-4954-8696-20948c6ff261.json",
              "--profile-dir=/tmp/ipy-be-jc1xlne9/profile_colab",
              "--profile=colab",
              "--ipython-dir=/tmp/ipy-be-1w__v67b",
              "--no-secure"
            ],
            "build": "Built on Mon Nov  3 09:03:03 2025 (1762189383)\nBuilt by rbex-enqueue-targets@oxgg22.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/google3\nBuilt as //research/colab/notebook:notebook_backend_py3\nBuild ID: a79b9e4c-6038-415d-b087-c265659a1b39\nBuilt from changelist 827500446 in a mint client based on //depot/google3\nBuild label: colab_runtime_default_runtime_20251103_0900_RC00\nBuild platform: gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\nBuild tool: Blaze, release blaze-2025.10.25-1 (mainline @823513786)\nBuilt with par options [\"--compress\", \"--compress_level=6\", \"--extra_strip=always\"]\nCurrently running under Python 3.12.11: embedded.\n",
            "user": "colaboratory-playground"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Setup data for the lab. We will be creating 1 Big dataset and another small dataset\n",
        "\n",
        "We create a large Fact table Employees with 5 million rows and a small Dimension table Regions with 500 rows. This size difference is key for testing join strategies."
      ],
      "metadata": {
        "id": "J9AxG1A1YQRT"
      },
      "id": "J9AxG1A1YQRT"
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_EMPLOYEES = 5000000\n",
        "TOTAL_REGIONS = 500\n",
        "\n",
        "# Create Region (Dimension) Table\n",
        "df_region = spark.range(1, TOTAL_REGIONS + 1).withColumnRenamed(\"id\", \"RegionID\") \\\n",
        "    .withColumn(\"RegionName\", F.concat(F.lit(\"Rgn_\"), F.col(\"RegionID\")))\n",
        "\n",
        "# Create Employee (Fact) Table\n",
        "df_emp = spark.range(TOTAL_EMPLOYEES).withColumnRenamed(\"id\", \"EmployeeID\") \\\n",
        "    .withColumn(\"Salary\", F.round(F.rand() * 120000)) \\\n",
        "    .withColumn(\"TransactionDate\", F.date_sub(F.current_date(), (F.rand() * 1000).cast(\"int\"))) \\\n",
        "    .withColumn(\"RegionID\", (F.rand() * TOTAL_REGIONS + 1).cast(\"int\"))\n",
        "df_emp = df_emp.withColumn(\"Notes\", F.lit(\"Long string data field that increases I/O size\"))\n",
        "\n",
        "# Create temporary views\n",
        "df_region.createOrReplaceTempView(\"region_raw\")\n",
        "df_emp.repartition(100,F.col(\"RegionID\")).createOrReplaceTempView(\"employee_raw_for_write\")\n",
        "\n",
        "print(f\"Employee Fact Table rows simulated: {TOTAL_EMPLOYEES}\")"
      ],
      "metadata": {
        "id": "RY4pNUFOYFha"
      },
      "id": "RY4pNUFOYFha",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create NameSpace"
      ],
      "metadata": {
        "id": "8lhxItGrY9Ee"
      },
      "id": "8lhxItGrY9Ee"
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"USE {CATALOG_NAME};\")\n",
        "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {NAMESPACE};\")\n",
        "spark.sql(f\"USE {NAMESPACE};\")"
      ],
      "metadata": {
        "id": "kbLCfX4wYpVn"
      },
      "id": "kbLCfX4wYpVn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create Source Tables in Iceberg"
      ],
      "metadata": {
        "id": "pat7pbsNZfCD"
      },
      "id": "pat7pbsNZfCD"
    },
    {
      "cell_type": "code",
      "source": [
        "CATALOG_PATH = f\"{CATALOG_NAME}.{NAMESPACE}\"\n",
        "EMPLOYEE_TABLE = f\"{CATALOG_PATH}.employees\"\n",
        "REGION_TABLE = f\"{CATALOG_PATH}.regions\"\n",
        "\n",
        "# 1. Write the large Fact table, PARTITIONED by RegionID.\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE OR REPLACE TABLE {EMPLOYEE_TABLE}\n",
        "    USING iceberg\n",
        "    PARTITIONED BY (RegionID)\n",
        "    AS SELECT * FROM employee_raw_for_write\n",
        "\"\"\")\n",
        "print(f\"Fact Table 1: Partitioned Iceberg created: {EMPLOYEE_TABLE}\")\n",
        "\n",
        "# 2. Write the small Dimension table\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE OR REPLACE TABLE {REGION_TABLE}\n",
        "    USING iceberg\n",
        "    AS SELECT * FROM region_raw\n",
        "\"\"\")\n",
        "print(f\"Region Dimension Table created: {REGION_TABLE}\")"
      ],
      "metadata": {
        "id": "fSIF-TbmY-xp"
      },
      "id": "fSIF-TbmY-xp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1: Join Strategy Tuning\n",
        "\n",
        "This module compares the performance of the default Shuffle Sort-Merge Join against the optimized Broadcast Hash Join for a Fact-Dimension join."
      ],
      "metadata": {
        "id": "M2WTN-oKZ-Se"
      },
      "id": "M2WTN-oKZ-Se"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Shuffle Sort-Merge Join\n",
        "\n",
        "SMJ is Spark's default join for large tables where the smaller table cannot be broadcast. It requires Spark to:\n",
        "\n",
        "- Shuffle: Move data from both tables across the network to group rows by the join key.\n",
        "\n",
        "- Sort: Sort the shuffled data within each partition.\n",
        "\n",
        "- Merge: Iterate through both sorted datasets in parallel to find matching keys. This process is highly I/O and CPU intensive, making it the slower baseline.\n",
        "\n",
        "After running this query, check the Spark UI. The DAG Visualization should show a SortMergeJoin operator.\n",
        "\n"
      ],
      "metadata": {
        "id": "8BHgHNAqawzD"
      },
      "id": "8BHgHNAqawzD"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting SMJ baseline...\")\n",
        "smj_join_baseline = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        e.EmployeeID, r.RegionName\n",
        "    FROM\n",
        "        {EMPLOYEE_TABLE} e\n",
        "    INNER JOIN\n",
        "        {REGION_TABLE} r ON e.RegionID = r.RegionID\n",
        "\"\"\").count()\n",
        "\n",
        "print(f\"SMJ Join Count: {smj_join_baseline}\")"
      ],
      "metadata": {
        "id": "Mw-bfvRMZkXl"
      },
      "id": "Mw-bfvRMZkXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Broadcast Hash Join\n",
        "\n",
        "BHJ is an optimization that is applicable when one table is significantly smaller (e.g., < 10MB by default, or fits in executor memory). Spark takes the smaller table, collects it onto the driver, and then broadcasts it to all executors. The join is then performed in memory using a hash map on each executor, entirely eliminating the expensive Shuffle and Sort phases.\n",
        "\n",
        "The code below uses the /*+ BROADCAST(r) */ hint to force Spark to broadcast the regions table\n",
        "\n",
        "The DAG Visualization should now show a BroadcastHashJoin operator, and you should observe a significantly faster execution time."
      ],
      "metadata": {
        "id": "F4cm07-ma_-z"
      },
      "id": "F4cm07-ma_-z"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting BHJ optimized join...\")\n",
        "\n",
        "bhj_join_optimized = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        /*+ BROADCAST(r) */ e.EmployeeID, r.RegionName\n",
        "    FROM\n",
        "        {EMPLOYEE_TABLE} e\n",
        "    INNER JOIN\n",
        "        {REGION_TABLE} r ON e.RegionID = r.RegionID\n",
        "\"\"\").count()\n",
        "\n",
        "print(f\"BHJ Optimized Count: {bhj_join_optimized}\")"
      ],
      "metadata": {
        "id": "7eKdCMDPa0_y"
      },
      "id": "7eKdCMDPa0_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see runtime difference try increasing employee dataset to 500M rows and run the queries again"
      ],
      "metadata": {
        "id": "lFTBh5l_W-9W"
      },
      "id": "lFTBh5l_W-9W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: Iceberg File Compaction\n",
        "\n",
        "This module demonstrates file compaction in Iceberg. Continuous writes (especially small, frequent inserts) create many small data files, which can lead to slow query performance because the query engine has to read much more metadata and initiate more I/O requests. Compaction merges these small files into larger, optimal-sized files."
      ],
      "metadata": {
        "id": "6srscAecfDih"
      },
      "id": "6srscAecfDih"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Create Table"
      ],
      "metadata": {
        "id": "-x8yNAtbgpGf"
      },
      "id": "-x8yNAtbgpGf"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create NameSpace---\n",
        "spark.sql(f\"USE {CATALOG_NAME};\")\n",
        "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {NAMESPACE};\")\n",
        "spark.sql(f\"USE {NAMESPACE};\")"
      ],
      "metadata": {
        "id": "12-b2F6UdYHV"
      },
      "id": "12-b2F6UdYHV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"\n",
        "    CREATE OR REPLACE TABLE Users (\n",
        "        user_id INT,\n",
        "        status STRING,\n",
        "        joined_date DATE\n",
        "    )\n",
        "    USING iceberg\n",
        "    PARTITIONED BY (joined_date)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "wLpQAbf5fHxW"
      },
      "id": "wLpQAbf5fHxW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Insert Data to Iceberg Table\n",
        "\n",
        "We perform 5 separate INSERT INTO statements. Since each statement is a separate transaction, it creates 5 distinct small data files."
      ],
      "metadata": {
        "id": "fN57hVzNhH_I"
      },
      "id": "fN57hVzNhH_I"
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"\n",
        "    Insert Into Users (\n",
        "        user_id,\n",
        "        status,\n",
        "        joined_date\n",
        "    )\n",
        "    Values\n",
        "    (\n",
        "      1, 'ACTIVE',DATE '2025-11-03'\n",
        "    )\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "p3yDPwItfS1V"
      },
      "id": "p3yDPwItfS1V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"\n",
        "    Insert Into Users (\n",
        "        user_id,\n",
        "        status,\n",
        "        joined_date\n",
        "    )\n",
        "    Values\n",
        "    (\n",
        "      2, 'ACTIVE',DATE '2025-11-03'\n",
        "    )\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "lpETCXWjfn4C"
      },
      "id": "lpETCXWjfn4C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"\n",
        "    Insert Into Users (\n",
        "        user_id,\n",
        "        status,\n",
        "        joined_date\n",
        "    )\n",
        "    Values\n",
        "    (\n",
        "      3, 'ACTIVE',DATE '2025-11-03'\n",
        "    )\n",
        "\n",
        "\"\"\")\n",
        "spark.sql(f\"\"\"\n",
        "    Insert Into Users (\n",
        "        user_id,\n",
        "        status,\n",
        "        joined_date\n",
        "    )\n",
        "    Values\n",
        "    (\n",
        "      4, 'ACTIVE',DATE '2025-11-03'\n",
        "    )\n",
        "\n",
        "\"\"\")\n",
        "spark.sql(f\"\"\"\n",
        "    Insert Into Users (\n",
        "        user_id,\n",
        "        status,\n",
        "        joined_date\n",
        "    )\n",
        "    Values\n",
        "    (\n",
        "      5, 'ACTIVE',DATE '2025-11-03'\n",
        "    )\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "n0KA9CNifueJ"
      },
      "id": "n0KA9CNifueJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Rewrite Data Files\n",
        "\n",
        "The rewrite_data_files stored procedure is an Iceberg maintenance operation. It reads the small data files that meet the where condition and merges their contents into a set of new, larger, optimized files.\n",
        "\n",
        "In below code we will compact files from a specific parition"
      ],
      "metadata": {
        "id": "N9FBlr6shMmb"
      },
      "id": "N9FBlr6shMmb"
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"CALL my_catalog.system.rewrite_data_files(\n",
        "    table => 'Users',\n",
        "    options => map('rewrite-all', 'true'),\n",
        "    where => \"joined_date >= '2025-11-03 00:00:00' AND joined_date < '2025-11-04 00:00:00'\"\n",
        ")\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "DbDKsFO0fxey"
      },
      "id": "DbDKsFO0fxey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3: Streaming Pipeline\n",
        "\n",
        "This module demonstrates building a real-time streaming pipeline using Spark Structured Streaming and Iceberg's transactional MERGE INTO operation to perform UPSERTs."
      ],
      "metadata": {
        "id": "4XtbAwefbvLf"
      },
      "id": "4XtbAwefbvLf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Define UPSERT Logic and Target Table\n",
        "\n",
        "Spark Structured Streaming processes data in small micro-batches. The foreachBatch function allows you to apply operations like complex SQL DML statements on the data within each micro-batch."
      ],
      "metadata": {
        "id": "pIvBkNovcNlM"
      },
      "id": "pIvBkNovcNlM"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Define the target table name for streaming\n",
        "STREAM_TARGET_TABLE = \"realtime_users1\"\n",
        "CHECKPOINT_LOCATION = f\"{CHECKPOINT_DIR}/user_activity_stream\"\n",
        "\n",
        "def upsert_to_iceberg(updates_df, batch_id):\n",
        "    \"\"\"\n",
        "    Performs a transactional MERGE INTO operation for each micro-batch,\n",
        "    including a deduplication step using a window function.\n",
        "    \"\"\"\n",
        "    if updates_df.count() == 0:\n",
        "        print(f\"Batch {batch_id}: No new records, skipping merge.\")\n",
        "        return\n",
        "\n",
        "    executor_spark = updates_df.sparkSession\n",
        "\n",
        "    # L300 Check: Deduplicate incoming micro-batch on user_id, prioritizing latest timestamp\n",
        "    window_spec = Window.partitionBy(\"user_id\").orderBy(F.col(\"timestamp\").desc())\n",
        "\n",
        "    deduplicated_updates_df = updates_df \\\n",
        "        .withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
        "        .filter(F.col(\"rank\") == 1) \\\n",
        "        .drop(\"rank\")\n",
        "\n",
        "    # Safety Check\n",
        "    if deduplicated_updates_df.isEmpty():\n",
        "        print(f\"Batch {batch_id}: Records dropped during deduplication, skipping merge.\")\n",
        "        return\n",
        "\n",
        "    deduplicated_updates_df.createOrReplaceTempView(\"stream_updates\")\n",
        "\n",
        "    print(f\"Batch {batch_id}: Merging {deduplicated_updates_df.count()} unique records into {STREAM_TARGET_TABLE}...\")\n",
        "\n",
        "    try:\n",
        "        # Execute the transactional UPSERT (MERGE INTO)\n",
        "        executor_spark.sql(f\"\"\"\n",
        "          MERGE INTO {STREAM_TARGET_TABLE} T\n",
        "          USING stream_updates S\n",
        "          ON T.user_id = S.user_id\n",
        "          WHEN MATCHED AND T.timestamp < S.timestamp THEN\n",
        "            UPDATE SET T.status = S.status, T.timestamp = S.timestamp\n",
        "          WHEN NOT MATCHED THEN\n",
        "            INSERT (user_id, status, timestamp)\n",
        "            VALUES (S.user_id, S.status, S.timestamp)\n",
        "        \"\"\")\n",
        "        print(f\"Batch {batch_id}: Merge completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Batch {batch_id}: Error during merge: {e}\")\n"
      ],
      "metadata": {
        "id": "H9gARP3jbvx7"
      },
      "id": "H9gARP3jbvx7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create the initial, empty target Iceberg table\n",
        "\n",
        "We partition the target table by days(timestamp) to ensure efficient querying over time and faster MERGE INTO operations"
      ],
      "metadata": {
        "id": "g0xWM9MHdI-Y"
      },
      "id": "g0xWM9MHdI-Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create NameSpace---\n",
        "spark.sql(f\"USE {CATALOG_NAME};\")\n",
        "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {NAMESPACE};\")\n",
        "spark.sql(f\"USE {NAMESPACE};\")"
      ],
      "metadata": {
        "id": "6hcP37TCNWLm"
      },
      "id": "6hcP37TCNWLm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"\n",
        "    CREATE OR REPLACE TABLE {STREAM_TARGET_TABLE} (\n",
        "        user_id INT,\n",
        "        status STRING,\n",
        "        timestamp TIMESTAMP\n",
        "    )\n",
        "    USING iceberg\n",
        "    PARTITIONED BY (days(timestamp))\n",
        "\"\"\")\n",
        "print(f\"Real-time Iceberg target table '{STREAM_TARGET_TABLE}' created.\")\n"
      ],
      "metadata": {
        "id": "jcMm2THocyqW"
      },
      "id": "jcMm2THocyqW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Start and Monitor the Stream\n",
        "\n",
        "The stream uses the rate source to simulate incoming data at 5 rows per second. The processingTime='10 seconds' trigger ensures the foreachBatch (and thus the MERGE INTO) runs every 10 seconds.\n"
      ],
      "metadata": {
        "id": "cnb8c5z9dVOm"
      },
      "id": "cnb8c5z9dVOm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the stream input schema\n",
        "input_schema = StructType([\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"value\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read stream using a rate source for testing\n",
        "stream_df = spark.readStream \\\n",
        "    .format(\"rate\") \\\n",
        "    .option(\"rowsPerSecond\", 5) \\\n",
        "    .option(\"checkpointLocation\", CHECKPOINT_LOCATION) \\\n",
        "    .load()\n",
        "\n",
        "# Parse the incoming simulated data to match the target schema\n",
        "parsed_stream = stream_df \\\n",
        "  .withColumn(\"user_id\", (F.col(\"value\") % 5) + 100) \\\n",
        "  .withColumn(\"status\", F.when(F.col(\"value\") % 3 == 0, \"LOGGED_IN\").otherwise(\"ACTIVE\")) \\\n",
        "  .withColumn(\"timestamp\", F.current_timestamp()) \\\n",
        "  .select(\"user_id\", \"status\", \"timestamp\")\n",
        "\n",
        "\n",
        "# Start the streaming query\n",
        "streaming_query = parsed_stream.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .foreachBatch(upsert_to_iceberg) \\\n",
        "    .trigger(processingTime='10 seconds') \\\n",
        "    .option(\"checkpointLocation\", CHECKPOINT_LOCATION) \\\n",
        "    .queryName(\"Iceberg_UPSERT_Stream\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"Streaming query started. Data is now being processed every 10 seconds.\")"
      ],
      "metadata": {
        "id": "Ziw9G6EgdKQT"
      },
      "id": "Ziw9G6EgdKQT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run the cell below to check status and stop the stream when done ---\n",
        "\n",
        "# Check the status of the stream (Run this cell repeatedly)\n",
        "streaming_query.status"
      ],
      "metadata": {
        "id": "xx0ZCXvBdovN"
      },
      "id": "xx0ZCXvBdovN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell to query the final Iceberg table. The timestamp should increase over time"
      ],
      "metadata": {
        "id": "melQcjGb4q8R"
      },
      "id": "melQcjGb4q8R"
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"SELECT * FROM {STREAM_TARGET_TABLE}\").show(truncate=False)"
      ],
      "metadata": {
        "id": "d-3Km9xZd5Di"
      },
      "id": "d-3Km9xZd5Di",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streaming_query.stop()\n",
        "print(\"Streaming query stopped and session resources released.\")"
      ],
      "metadata": {
        "id": "YWziO6O4eITL"
      },
      "id": "YWziO6O4eITL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}