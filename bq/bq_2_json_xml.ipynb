{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEtVY8D4aZuC5m4z7kVMly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendanlooker/colab-examples/blob/main/bq/bq_2_json_xml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydCQm809wIST",
        "outputId": "04f1720c-c72e-44da-a78e-0d822c41cc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8exvlt6FumQw"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery,storage\n",
        "from datetime import datetime\n",
        "import json\n",
        "import xmltodict\n",
        "import gzip\n",
        "import io\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "\n",
        "# Get current timestamp\n",
        "current_datetime = datetime.now()\n",
        "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d:%H:%M:%S\")\n",
        "\n",
        "\n",
        "# Export Table from BQ to GCS\n",
        "def export_table_to_gcs_as_json(file_type,bucket_name):\n",
        "\n",
        "    global formatted_datetime\n",
        "\n",
        "    project_id = 'brendanlooker'\n",
        "    dataset_id = 'puma'\n",
        "    # bucket_name = 'puma-pim-datapipeline'\n",
        "    file_name = f'{file_type}-{formatted_datetime}'\n",
        "    destination_uri = f\"gs://{bucket_name}/AUS/bqExport/json/{file_name}.json\"\n",
        "\n",
        "\n",
        "    # Initialize a BigQuery client\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # Define the source table\n",
        "    table_ref = client.dataset(dataset_id).table(file_type)\n",
        "\n",
        "    # Create a job configuration\n",
        "    job_config = bigquery.job.ExtractJobConfig()\n",
        "    job_config.destination_format = bigquery.job.DestinationFormat.NEWLINE_DELIMITED_JSON\n",
        "\n",
        "    # Define the export job\n",
        "    extract_job = client.extract_table(\n",
        "        table_ref,\n",
        "        destination_uri,\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    # Start the job and wait for it to complete\n",
        "    extract_job.result()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     # Load the exported JSON data\n",
        "    with open(f\"AUS/bqExport/json/{file_name}.json\", 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Filter the JSON data\n",
        "    filtered_data = filter_json_data(data)\n",
        "\n",
        "    # Write the filtered JSON data back to the file\n",
        "    with open(f\"AUS/bqExport/json/{file_name}_filtered.json\", 'w') as f:\n",
        "        json.dump(filtered_data, f, indent=4)\n",
        "\n",
        "    return file_name\n",
        "\n",
        "# Convert json file to XML\n",
        "def convert_json_to_xml(gcs_file_name,bucket_name):\n",
        "\n",
        "    global formatted_datetime\n",
        "\n",
        "    # Export data as JSON function call\n",
        "    exported_file_name = gcs_file_name\n",
        "\n",
        "    # Initialise Client\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob_path = f'AUS/bqExport/json/{exported_file_name}.json'\n",
        "    blob = bucket.get_blob(blob_path)\n",
        "\n",
        "    # Decode the entire file content\n",
        "    file_content = blob.download_as_string().decode('utf-8')\n",
        "\n",
        "    # Define GCS destination object\n",
        "    write_xml = bucket.blob(f'AUS/bqExport/xml/{exported_file_name}.xml')\n",
        "\n",
        "\n",
        "    # Close Root Tag in XML file (<catalog> tag introduced in header string and therefore needs to be subsequently closed)\n",
        "    # Header is specific to the file type and is processed as appropriate below\n",
        "    footer = \"</catalog>\"\n",
        "\n",
        "\n",
        "    # Process Site Catalog\n",
        "    if gcs_file_name.startswith('site-catalog'):\n",
        "\n",
        "        # Hard-coded XML file header for Site Catalog\n",
        "        header = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n",
        "        <catalog xmlns=\"http://www.demandware.com/xml/impex/catalog/2006-10-31\" catalog-id=\"puma-catalog-autocat-au\">\n",
        "        \"\"\"\n",
        "\n",
        "        # Updates are required to the XML to align with Salesforce input requirements\n",
        "        replacements = [\n",
        "            ('<?xml version=\"1.0\" encoding=\"utf-8\"?>', ''),  # Remove XML declaration\n",
        "            ]\n",
        "\n",
        "    else: # Process Master Catalog\n",
        "\n",
        "        # Hard-coded XML file header for Master Catalog\n",
        "        header = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n",
        "        <catalog xmlns=\"http://www.demandware.com/xml/impex/catalog/2006-10-31\" catalog-id=\"puma-master-catalog\">\n",
        "            <header>\n",
        "                <image-settings>\n",
        "                    <external-location>\n",
        "                        <http-url>https://images.puma.net/images</http-url>\n",
        "                        <https-url>https://images.puma.net/images</https-url>\n",
        "                    </external-location>\n",
        "                    <view-types>\n",
        "                        <view-type>extralarge-AUS</view-type>\n",
        "                        <view-type>large-AUS</view-type>\n",
        "                        <view-type>medium-AUS</view-type>\n",
        "                        <view-type>small-AUS</view-type>\n",
        "                        <view-type>swatch-AUS</view-type>\n",
        "                    </view-types>\n",
        "                    <variation-attribute-id>color</variation-attribute-id>\n",
        "                    <alt-pattern>${productname}, ${variationvalue}, ${viewtype}</alt-pattern>\n",
        "                    <title-pattern>${productname}, ${variationvalue}</title-pattern>\n",
        "                </image-settings>\n",
        "            </header>\n",
        "        \"\"\"\n",
        "\n",
        "        # Updates are required to the XML to align with Salesforce input requirements\n",
        "        replacements = [\n",
        "            ('<?xml version=\"1.0\" encoding=\"utf-8\"?>', ''),  # Remove XML declaration\n",
        "            ('<product>', ''),                              # Remove <product> tag\n",
        "            ('</product>', ''),                             # Remove </product> tag\n",
        "            ('<product_style>', '<product>'),               # Replace <product_style> with <product>\n",
        "            ('<product_style ', '<product '),\n",
        "            ('</product_style>', '</product>'),             # Replace </product_style> with </product>\n",
        "            ('<product_size>', '<product>'),                # Replace <product_size> with <product>\n",
        "            ('</product_size>', '</product>'),               # Replace </product_size> with </product>\n",
        "            ('<image>',''),\n",
        "            ('</image>',''),\n",
        "            ('<path>','<image path=\"'),\n",
        "            ('</path>','\"/>'),\n",
        "            ('<image-group-master>',''),\n",
        "            ('</image-group-master>',''),\n",
        "            ('<product-id-variations>','<variant product-id=\"'),\n",
        "            ('</product-id-variations>','\"/>'),\n",
        "            ('<variant>',''),\n",
        "            ('</variant>',''),\n",
        "            ('<custom-attribute>',''),\n",
        "            ('</custom-attribute>',''),\n",
        "            ]\n",
        "\n",
        "        # List of Custom Attribute fields which require XML formatting changes\n",
        "        custom_attributes = ['activityGroup', 'ageCode', 'ageCodeGlobal', 'ageGlobal', 'ageGroup', 'Cushioning', 'articleType', 'articleTypeCode', 'apparelLength', 'bodyStyle1', 'bodyStyle2', 'collection', 'cupType', 'leatherType', 'mainMaterial', 'mainMaterialOfShell', 'deptCode', 'deptName', 'dimensions', 'gender', 'genderCode', 'hsCode1', 'hsCode2', 'productDivision', 'productdivCode', 'productgroupKey', 'productlineKey', 'refinementHTD', 'refinementLevel', 'refinementShoeType', 'refinementSilo', 'refinementSpecialFeature', 'refinementSurface', 'refinementSurfaceRunning', 'refinementWeight', 'sizetableNumber', 'sportCode', 'subcatID', 'taxClassCode', 'Neck', 'upper', 'classCode', 'className', 'sportName', 'subcatName', 'styleLogo', 'pocketType', 'productType', 'sleeves', 'supportLevelBra', 'team', 'trouserRise', 'waterResistant', 'weatherConditions', 'careInstructions', 'careSymbol', 'RunnerType', 'sportType', 'articleGroup', 'avgNoOfMilesPerKm', 'fabricsType', 'fastener', 'fit', 'franchise', 'heelToToeDrop', 'heelType', 'hood', 'lineName', 'pumaTechnology', 'refinementCushioningLevel', 'removableSole', 'shoePronation', 'surface', 'technologyPurpose', 'toeType', 'vegan', 'volume', 'sizeSpecTable', 'sizeSpecTableImp', 'mainColorID', 'season', 'taxClassCode', 'animalParts', 'color', 'colorName', 'countryOfOrigin', 'mainColorAUS', 'mainColorNZ', 'productHeight', 'productLength', 'productWeight', 'productWidth', 'refinementColor', 'secondColor', 'size', 'styleNumber', 'pattern', 'colorDescription', 'new', 'materialComposition']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Open XML file for writing\n",
        "    with write_xml.open(\"wt\") as file_obj:\n",
        "\n",
        "        # Add XML header\n",
        "        file_obj.write(header + \"\\n\")\n",
        "\n",
        "        # Split the content by line and parse each line as JSON\n",
        "        for line in file_content.splitlines():\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "\n",
        "                # Use xmltodict to generate an XML string from the JSON dict\n",
        "                xml_string = xmltodict.unparse(data, cdata_key='_VALUE',attr_prefix='_', pretty=True)\n",
        "\n",
        "                # Apply the replacements\n",
        "                for old, new in replacements:\n",
        "                    xml_string = xml_string.replace(old, new)\n",
        "\n",
        "                # Fotmat updates for Custom Attributes\n",
        "                if gcs_file_name.startswith('master-catalog'):\n",
        "                    for item in custom_attributes:\n",
        "                        if item in xml_string:\n",
        "                            xml_string = xml_string.replace('<'+item+' ', '<custom-attribute ')\n",
        "                            xml_string = xml_string.replace('</'+item+'>', '</custom-attribute>')\n",
        "\n",
        "                # Remove empty lines\n",
        "                if xml_string.strip():\n",
        "                    # Write data to GCS blob\n",
        "                    file_obj.write(xml_string + \"\\n\")\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(\"Error parsing JSON:\", e)\n",
        "\n",
        "        # Write the footer to finalise the file\n",
        "        file_obj.write(footer)\n",
        "\n",
        "    return f\"{exported_file_name}.xml\"\n",
        "\n",
        "def gzip_gcs_file(bucket_name, source_blob_name, destination_blob_name):\n",
        "    # Initialize GCS client\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    # Download file from GCS\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    content = blob.download_as_string()\n",
        "\n",
        "    # Compress file content\n",
        "    with io.BytesIO() as buf:\n",
        "        with gzip.GzipFile(fileobj=buf, mode='wb') as f:\n",
        "            f.write(content)\n",
        "        buf.seek(0)\n",
        "        compressed_content = buf.read()\n",
        "\n",
        "    # Upload compressed content back to GCS\n",
        "    compressed_blob = bucket.blob(destination_blob_name)\n",
        "    compressed_blob.upload_from_string(compressed_content)\n",
        "\n",
        "    print(f'File {source_blob_name} compressed and saved as {destination_blob_name} in {bucket_name}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def filter_json_data(data):\n",
        "    # Iterate over each row in the JSON data\n",
        "    for row in data:\n",
        "        # Filter out key-value pairs where the value is empty\n",
        "        filtered_row = {key: value for key, value in row.items() if value}\n",
        "\n",
        "        # Update the row with the filtered key-value pairs\n",
        "        row.clear()\n",
        "        row.update(filtered_row)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(request):\n",
        "    msg = ''\n",
        "    bucket_name = 'puma-pim-datapipeline'\n",
        "    for file_type in ['site-catalog','master-catalog']:\n",
        "      gcs_file_name = export_table_to_gcs_as_json(file_type, bucket_name)\n",
        "      # xml_filename = convert_json_to_xml(gcs_file_name, bucket_name)\n",
        "\n",
        "      # Compress the XML file\n",
        "      # xml_filename_zip = gzip_gcs_file(bucket_name,f\"AUS/bqExport/xml/{xml_filename}\", f\"AUS/bqExport/xml/{xml_filename}.gz\")\n",
        "      # # Remove the XML file\n",
        "      # os.remove(xml_filename)\n",
        "      # msg += xml_filename + ' / '\n",
        "    return {\"Success\": f\"XML export files created: {msg}\"}, 200\n",
        "\n",
        "main('run')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "import json\n",
        "\n",
        "def read_json_from_gcs(bucket_name, file_name):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "    json_data = blob.download_as_string().decode('utf-8')\n",
        "    json_objects = [json.loads(line) for line in json_data.strip().split('\\n')]\n",
        "    return json_objects\n",
        "\n",
        "def write_json_to_gcs(bucket_name, file_name, data):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Convert each JSON object to a string and concatenate with newline character\n",
        "    ndjson_data = '\\n'.join(json.dumps(obj) for obj in data)\n",
        "\n",
        "    # Upload the NDJSON data to GCS\n",
        "    blob.upload_from_string(ndjson_data, content_type='application/json')\n",
        "\n",
        "def export_table_to_gcs_as_json(file_type, bucket_name, formatted_datetime):\n",
        "    project_id = 'brendanlooker'\n",
        "    dataset_id = 'puma'\n",
        "    file_name = f'{file_type}-{formatted_datetime}'\n",
        "    destination_uri = f\"gs://{bucket_name}/AUS/bqExport/json/{file_name}.json\"\n",
        "\n",
        "    # Initialize a BigQuery client\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # Define the source table\n",
        "    table_ref = client.dataset(dataset_id).table(file_type)\n",
        "\n",
        "    # Create a job configuration\n",
        "    job_config = bigquery.job.ExtractJobConfig()\n",
        "    job_config.destination_format = bigquery.job.DestinationFormat.NEWLINE_DELIMITED_JSON\n",
        "\n",
        "    # Define the export job\n",
        "    extract_job = client.extract_table(\n",
        "        table_ref,\n",
        "        destination_uri,\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    # Start the job and wait for it to complete\n",
        "    extract_job.result()\n",
        "\n",
        "    # Read the exported JSON data from GCS\n",
        "    data = read_json_from_gcs(bucket_name, f\"AUS/bqExport/json/{file_name}.json\")\n",
        "\n",
        "    # Filter the JSON data\n",
        "    filtered_data = filter_json_data(data)\n",
        "\n",
        "    # Write the filtered JSON data back to GCS\n",
        "    filtered_file_name = f\"{file_name}_filtered.json\"\n",
        "    write_json_to_gcs(bucket_name, f\"AUS/bqExport/json/{filtered_file_name}\", filtered_data)\n",
        "\n",
        "    return filtered_file_name\n",
        "\n",
        "def filter_json_data(data):\n",
        "    # Iterate over each row in the JSON data\n",
        "    for row in data:\n",
        "        # Filter out key-value pairs where the value is empty\n",
        "        filtered_row = {key: value for key, value in row.items() if value}\n",
        "\n",
        "        # Update the row with the filtered key-value pairs\n",
        "        row.clear()\n",
        "        row.update(filtered_row)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "file_type = 'master-catalog'\n",
        "bucket_name = 'puma-pim-datapipeline'\n",
        "formatted_datetime = '2021-01-01'\n",
        "\n",
        "filtered_file_name = export_table_to_gcs_as_json(file_type, bucket_name, formatted_datetime)\n"
      ],
      "metadata": {
        "id": "sFkFAFkr6AzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery,storage\n",
        "from datetime import datetime\n",
        "import json\n",
        "import xmltodict\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Get current timestamp\n",
        "current_datetime = datetime.now()\n",
        "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d:%H:%M:%S\")\n",
        "\n",
        "\n",
        "# Export Table from BQ to GCS\n",
        "def export_table_to_gcs_as_json(file_type):\n",
        "\n",
        "    global formatted_datetime\n",
        "\n",
        "    project_id = 'brendanlooker'\n",
        "    dataset_id = 'puma'\n",
        "    bucket_name = 'puma-pim-datapipeline'\n",
        "    file_name = f'{file_type}-{formatted_datetime}'\n",
        "    destination_uri = f\"gs://{bucket_name}/AUS/bqExport/json/{file_name}.json\"\n",
        "\n",
        "\n",
        "    # Initialize a BigQuery client\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # Define the source table\n",
        "    table_ref = client.dataset(dataset_id).table(file_type)\n",
        "\n",
        "    # Create a job configuration\n",
        "    job_config = bigquery.job.ExtractJobConfig()\n",
        "    job_config.destination_format = bigquery.job.DestinationFormat.NEWLINE_DELIMITED_JSON\n",
        "\n",
        "    # Define the export job\n",
        "    extract_job = client.extract_table(\n",
        "        table_ref,\n",
        "        destination_uri,\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    # Start the job and wait for it to complete\n",
        "    extract_job.result()\n",
        "\n",
        "    return file_name\n",
        "\n",
        "# Convert json file to XML\n",
        "def convert_json_to_xml(gcs_file_name):\n",
        "\n",
        "    global formatted_datetime\n",
        "\n",
        "    # Export data as JSON function call\n",
        "    exported_file_name = gcs_file_name\n",
        "\n",
        "    # Initialise Client\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket('puma-pim-datapipeline')\n",
        "    blob_path = f'AUS/bqExport/json/{exported_file_name}.json'\n",
        "    blob = bucket.get_blob(blob_path)\n",
        "\n",
        "    # Decode the entire file content\n",
        "    file_content = blob.download_as_string().decode('utf-8')\n",
        "\n",
        "    # Define GCS destination object\n",
        "    write_xml = bucket.blob(f'AUS/bqExport/xml/{exported_file_name}.xml')\n",
        "\n",
        "\n",
        "    # Close Root Tag in XML file (<catalog> tag introduced in header string and therefore needs to be subsequently closed)\n",
        "    # Header is specific to the file type and is processed as appropriate below\n",
        "    footer = \"</catalog>\"\n",
        "\n",
        "\n",
        "    # Process Site Catalog\n",
        "    if gcs_file_name.startswith('site-catalog'):\n",
        "\n",
        "        # Hard-coded XML file header for Site Catalog\n",
        "        header = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n",
        "        <catalog xmlns=\"http://www.demandware.com/xml/impex/catalog/2006-10-31\" catalog-id=\"puma-catalog-autocat-au\">\n",
        "        \"\"\"\n",
        "\n",
        "        # Updates are required to the XML to align with Salesforce input requirements\n",
        "        replacements = [\n",
        "            ('<?xml version=\"1.0\" encoding=\"utf-8\"?>', ''),  # Remove XML declaration\n",
        "            ]\n",
        "\n",
        "    else: # Process Master Catalog\n",
        "\n",
        "        # Hard-coded XML file header for Master Catalog\n",
        "        header = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n",
        "        <catalog xmlns=\"http://www.demandware.com/xml/impex/catalog/2006-10-31\" catalog-id=\"puma-master-catalog\">\n",
        "            <header>\n",
        "                <image-settings>\n",
        "                    <external-location>\n",
        "                        <http-url>https://images.puma.net/images</http-url>\n",
        "                        <https-url>https://images.puma.net/images</https-url>\n",
        "                    </external-location>\n",
        "                    <view-types>\n",
        "                        <view-type>extralarge-AUS</view-type>\n",
        "                        <view-type>large-AUS</view-type>\n",
        "                        <view-type>medium-AUS</view-type>\n",
        "                        <view-type>small-AUS</view-type>\n",
        "                        <view-type>swatch-AUS</view-type>\n",
        "                    </view-types>\n",
        "                    <variation-attribute-id>color</variation-attribute-id>\n",
        "                    <alt-pattern>${productname}, ${variationvalue}, ${viewtype}</alt-pattern>\n",
        "                    <title-pattern>${productname}, ${variationvalue}</title-pattern>\n",
        "                </image-settings>\n",
        "            </header>\n",
        "        \"\"\"\n",
        "\n",
        "        # Updates are required to the XML to align with Salesforce input requirements\n",
        "        replacements = [\n",
        "            ('<?xml version=\"1.0\" encoding=\"utf-8\"?>', ''),  # Remove XML declaration\n",
        "            ('<product>', ''),                              # Remove <product> tag\n",
        "            ('</product>', ''),                             # Remove </product> tag\n",
        "            ('<product_style>', '<product>'),               # Replace <product_style> with <product>\n",
        "            ('<product_style ', '<product '),\n",
        "            ('</product_style>', '</product>'),             # Replace </product_style> with </product>\n",
        "            ('<product_size>', '<product>'),                # Replace <product_size> with <product>\n",
        "            ('</product_size>', '</product>'),               # Replace </product_size> with </product>\n",
        "            ('<image>',''),\n",
        "            ('</image>',''),\n",
        "            ('<path>','<image path=\"'),\n",
        "            ('</path>','\"/>'),\n",
        "            ('<image-group-master>',''),\n",
        "            ('</image-group-master>',''),\n",
        "            ('<product-id-variations>','<variant product-id=\"'),\n",
        "            ('</product-id-variations>','\"/>'),\n",
        "            ('<variant>',''),\n",
        "            ('</variant>',''),\n",
        "            ('<custom-attribute>',''),\n",
        "            ('</custom-attribute>',''),\n",
        "            ]\n",
        "\n",
        "        # List of Custom Attribute fields which require XML formatting changes\n",
        "        custom_attributes = ['activityGroup', 'ageCode', 'ageCodeGlobal', 'ageGlobal', 'ageGroup', 'Cushioning', 'articleType', 'articleTypeCode', 'apparelLength', 'bodyStyle1', 'bodyStyle2', 'collection', 'cupType', 'leatherType', 'mainMaterial', 'mainMaterialOfShell', 'deptCode', 'deptName', 'dimensions', 'gender', 'genderCode', 'hsCode1', 'hsCode2', 'productDivision', 'productdivCode', 'productgroupKey', 'productlineKey', 'refinementHTD', 'refinementLevel', 'refinementShoeType', 'refinementSilo', 'refinementSpecialFeature', 'refinementSurface', 'refinementSurfaceRunning', 'refinementWeight', 'sizetableNumber', 'sportCode', 'subcatID', 'taxClassCode', 'Neck', 'upper', 'classCode', 'className', 'sportName', 'subcatName', 'styleLogo', 'pocketType', 'productType', 'sleeves', 'supportLevelBra', 'team', 'trouserRise', 'waterResistant', 'weatherConditions', 'careInstructions', 'careSymbol', 'RunnerType', 'sportType', 'articleGroup', 'avgNoOfMilesPerKm', 'fabricsType', 'fastener', 'fit', 'franchise', 'heelToToeDrop', 'heelType', 'hood', 'lineName', 'pumaTechnology', 'refinementCushioningLevel', 'removableSole', 'shoePronation', 'surface', 'technologyPurpose', 'toeType', 'vegan', 'volume', 'sizeSpecTable', 'sizeSpecTableImp', 'mainColorID', 'season', 'taxClassCode', 'animalParts', 'color', 'colorName', 'countryOfOrigin', 'mainColorAUS', 'mainColorNZ', 'productHeight', 'productLength', 'productWeight', 'productWidth', 'refinementColor', 'secondColor', 'size', 'styleNumber', 'pattern', 'colorDescription', 'new', 'materialComposition']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Open XML file for writing\n",
        "    with write_xml.open(\"wt\") as file_obj:\n",
        "\n",
        "        # Add XML header\n",
        "        file_obj.write(header + \"\\n\")\n",
        "\n",
        "        # Split the content by line and parse each line as JSON\n",
        "        for line in file_content.splitlines():\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "\n",
        "                # Use xmltodict to generate an XML string from the JSON dict\n",
        "                xml_string = xmltodict.unparse(data, cdata_key='_VALUE',attr_prefix='_', pretty=True)\n",
        "\n",
        "                # Apply the replacements\n",
        "                for old, new in replacements:\n",
        "                    xml_string = xml_string.replace(old, new)\n",
        "\n",
        "                # Fotmat updates for Custom Attributes\n",
        "                if gcs_file_name.startswith('master-catalog'):\n",
        "                    for item in custom_attributes:\n",
        "                        if item in xml_string:\n",
        "                            xml_string = xml_string.replace('<'+item+' ', '<custom-attribute ')\n",
        "                            xml_string = xml_string.replace('</'+item+'>', '</custom-attribute>')\n",
        "\n",
        "                # Remove empty lines\n",
        "                if xml_string.strip():\n",
        "                    # Write data to GCS blob\n",
        "                    file_obj.write(xml_string + \"\\n\")\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(\"Error parsing JSON:\", e)\n",
        "\n",
        "        # Write the footer to finalise the file\n",
        "        file_obj.write(footer)\n",
        "\n",
        "    return f\"{exported_file_name}.xml\"\n",
        "\n",
        "\n",
        "\n",
        "def main(request):\n",
        "    msg = ''\n",
        "    for file_type in ['site-catalog','master-catalog']:\n",
        "      gcs_file_name = export_table_to_gcs_as_json(file_type)\n",
        "      xml_filename = convert_json_to_xml(gcs_file_name)\n",
        "      msg += xml_filename + ' / '\n",
        "    return {\"Success\": f\"XML export files created: {msg}\"}, 200\n",
        "\n",
        "main('run')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41JpV2JSBMYl",
        "outputId": "f2176e6c-a947-4b2d-a1ca-b01aef730a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Success': 'XML export files created: site-catalog-2024-02-14:14:32:10.xml / master-catalog-2024-02-14:14:32:10.xml / '},\n",
              " 200)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}